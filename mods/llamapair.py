from mods.llama import Llama
import threading
from typing import Callable

class LlamaPair:
    def __init__(self, llama1: Llama, llama2: Llama, onResponse: Callable = lambda msg, res: None):
        """
        Initializes a LlamaPair instance with two Llama objects.
        Args:
            llama1 (Llama): The first Llama object.
            llama2 (Llama): The second Llama object.
            onResponse (Callable, optional): A callback function to handle responses. Defaults to a no-op lambda function. 
        Attributes:
            llamas (list): A list containing the two Llama objects.
            llama_index (int): An index to track the current Llama, initialized to 0.
        """
        
        self.llamas = [llama1, llama2]
        self.llama_index = 0
        self.onResponse = onResponse
        
    
    def send(self, message: str) -> str:
        """
        Sends a message to the next Llama in the conversation sequence and returns its response.
        Args:
            message (str): The message to be sent to the next Llama.
        Returns:
            str: The response generated by the next Llama.
        Behavior:
            - The method retrieves the current and next Llama instances from `_reactiveLlamaTurns`.
            - It generates a functional message for the current Llama with the role "assistant".
            - If the generated message is not already in the current Llama's message list, it appends it.
            - The message is then sent to the next Llama with the role "user", and the response is saved.
        """
        
        currentLlama, nextLlama = self._reactiveLlamaTurns
        functionalMessage = Llama.generateMessage(message, role="assistant")
        
        if functionalMessage not in currentLlama.messages:
            currentLlama.messages.append(functionalMessage)
        
        response = nextLlama.send(message, role="user", save=True)
        
        return response
    
    def sendReactive(self, message: str) -> None:
        """
        Sends a message reactively in a separate thread and handles the response.
        This method spawns a new thread to execute the inner function, which sends
        a message to the current Llama, retrieves the response, and triggers the
        `onResponse` callback with the (sender, message) and (receiver, response) arguments.
        Args:
            message (str): The message to be sent to the current Llama.
        Returns:
            None
        """
        
        def inner(message: str) -> None:
            currentLlama = self.currentLlama
            nextLlama = self.nextLlama
            response = self.send(message)
            self.onResponse((currentLlama, message), (nextLlama, response))
            
            return None
        
        threading.Thread(target=inner, args=(message,)).start()

    
    
    def skipTurn(self) -> None:
        """
        Skips the current turn by incrementing the llama index.
        This method updates the `llama_index` attribute to point to the next
        item or state, effectively skipping the current turn.
        """
        
        self.llama_index += 1
    
    
    @property
    def currentLlama(self) -> Llama:
        return self.llamas[self.llama_index]
    
    @property
    def nextLlama(self) -> Llama:
        return self.llamas[(self.llama_index+1)%2]

    
    @property
    def _reactiveLlamaTurns(self) -> list[Llama]:
        """
        Retrieves the current llama and determines the next llama in the sequence.
        This method alternates between two llamas in the `llamas` list based on the
        current `llama_index`. It returns a list containing the current llama and
        the next llama in the sequence.
        Returns:
            list[Llama]: A list containing the current llama and the next llama.
        """
        currentLlama = self.llamas[self.llama_index]
        
        self.llama_index = (self.llama_index + 1) % 2
        nextLlama = self.llamas[self.llama_index]
        
        return [currentLlama, nextLlama] 