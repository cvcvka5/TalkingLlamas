from mods.llama import Llama


class LlamaPair:
    def __init__(self, llama1: Llama, llama2: Llama):
        """
        Initializes a LlamaPair instance with two Llama objects.
        Args:
            llama1 (Llama): The first Llama object.
            llama2 (Llama): The second Llama object.
        Attributes:
            llamas (list): A list containing the two Llama objects.
            llama_index (int): An index to track the current Llama, initialized to 0.
        """
        
        self.llamas = [llama1, llama2]
        self.llama_index = 0
        
    
    def send(self, message: str) -> str:
        """
        Sends a message to the next Llama in the conversation sequence and returns its response.
        Args:
            message (str): The message to be sent to the next Llama.
        Returns:
            str: The response generated by the next Llama.
        Behavior:
            - The method retrieves the current and next Llama instances from `_reactiveLlamaTurns`.
            - It generates a functional message for the current Llama with the role "assistant".
            - If the generated message is not already in the current Llama's message list, it appends it.
            - The message is then sent to the next Llama with the role "user", and the response is saved.
        """
        
        currentLlama, nextLlama = self._reactiveLlamaTurns
        functionalMessage = Llama.generateMessage(message, role="assistant")
        
        if functionalMessage not in currentLlama.messages:
            currentLlama.messages.append(functionalMessage)
        
        response = nextLlama.send(message, role="user", save=True)
        
        return response
        
    def skipTurn(self) -> None:
        """
        Skips the current turn by incrementing the llama index.
        This method updates the `llama_index` attribute to point to the next
        item or state, effectively skipping the current turn.
        """
        
        self.llama_index += 1
    
    
    @property
    def currentLlama(self) -> Llama:
        return self.llamas[self.llama_index]
    
    @property
    def nextLlama(self) -> Llama:
        return self.llamas[(self.llama_index+1)%2]

    
    @property
    def _reactiveLlamaTurns(self) -> list[Llama]:
        """
        Retrieves the current llama and determines the next llama in the sequence.
        This method alternates between two llamas in the `llamas` list based on the
        current `llama_index`. It returns a list containing the current llama and
        the next llama in the sequence.
        Returns:
            list[Llama]: A list containing the current llama and the next llama.
        """
        currentLlama = self.llamas[self.llama_index]
        
        self.llama_index = (self.llama_index + 1) % 2
        nextLlama = self.llamas[self.llama_index]
        
        return [currentLlama, nextLlama] 